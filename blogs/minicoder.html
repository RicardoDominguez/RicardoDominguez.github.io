<?xml version="1.0" encoding="UTF-8"?>
<html>
<head>
  <title>mini-coder</title>
  <style type="text/css">
    body {
      text-align: center;
      max-width: 800px;
      margin: 0 auto;
      line-height: 1.6;
    }
    p {
      margin: 1em auto;
      max-width:80ch; /* Limits line length to approximately 80 characters */
      text-align: justify;
    }
  </style>
</head>
<body>
<span class="style1">
<br>
<br>
<h1 style="font-weight:590; font-size:1.8em;">
    <code style="font-size:1.1em; font-weight:600;">mini-coder</code>: small models for agentic SWE research
</h1>  
<h2 style="font-weight: normal; font-size: 1.3em; margin-bottom: -1.0em;">Ricardo Olmedo</h2>
<h3 style="font-weight: normal; font-size: 1.2em; margin-bottom: -1.0em;">Max Planck Institute for Intelligent Systems, Tübingen<br></h3>
<h3 style="font-weight: normal; font-size: 1.2em; margin-bottom: 0.0em;">September 30, 2025<br></h3>
<br>
<i style="font-size: 1.2em;">Two distilled models (1.7B & 4B) + 400k training trajectories from Qwen 3 Coder.</i>
<br>
<i style="font-size: 1.2em;">Download the models & dataset <a href="https://huggingface.co/collections/ricdomolm/mini-coder-68dbc8198dd63ff63ed8ef7d">here.</a></i>
<div class="centered-text" style="max-width: 67ch; margin: 0 auto;">
</div>
<!-- &quot; -->
<p>
<br>
<table align="center" cellspacing="0" cellpadding="6">
    <caption><b>SWE-bench Verified (Bash only)</b></caption>
    <thead>
        <tr>
        <th></th>
        <th>pass@1</th>
        <th>pass@100</th>
        </tr>
    </thead>
    <tbody>
        <tr>
        <td>Qwen 3 Coder 30B-A3B</td>
        <td align="right">33.2</td>
        <td align="right">67.4</td>
        </tr>
        <tr>
        <td><b style="font-size: 1.2em;"><code>mini-swe-4b</code></b></td>
        <td align="right">26.8</td>
        <td align="right">60.2</td>
        </tr>
        <tr>
        <td>gpt-oss-120b</td>
        <td align="right">26.0</td>
        <td align="right">-</td>
        </tr>
        <tr>
        <td><b style="font-size: 1.2em;"><code>mini-swe-1.7b</code></b></td>
        <td align="right">18.6</td>
        <td align="right">50.4</td>
        </tr>
        <tr>
        <td>SWE-agent-LM 7B</td>
        <td align="right">15.2</td>
        <td align="right">-</td>
        </tr>
        <tr>
        <td>Qwen 3 4B Instruct 2507</td>
        <td align="right">4.0</td>
        <td align="right">25.1</td>
        </tr>
    </tbody>
</table>
<br>
<br>
Small models play a crucial role in today's research ecosystem. They enable a larger pool of researchers to contribute to the field, thereby accelerating scientific progress. Unfortunately, the entry barrier for agentic SWE research remains high: performant open-weight models are in the 30B-parameter range. To make matters worse, SWE agentic tasks require long, multi-turn interactions, further increasing GPU memory demands. As a result, research on post-training SWE agents generally requires multi-GPU—and often multi-node—setups.
</p><p>
To lower this entry barrier, we trained <code><a href="https://huggingface.co/collections/ricdomolm/mini-coder-68dbc8198dd63ff63ed8ef7d">mini-coder</a></code>: two small but performant agentic SWE models. We follow a straightforward training recipe: distillation from a larger, more capable model. We distill from <a href="https://qwenlm.github.io/blog/qwen3-coder/">Qwen 3 Coder 30B</a>, which strikes a good balance between performance and inference cost. Using the <a href="https://arxiv.org/abs/2504.21798">SWE-smith</a> dataset of GitHub issues, together with the lightweight <a href="https://mini-swe-agent.com/latest/">mini-swe-agent</a> scaffolding, we generated 400k training trajectories (~5.5B tokens). We then fine-tuned Qwen&nbsp;3&nbsp;1.7B and Qwen&nbsp;3&nbsp;4B Instruct on these trajectories.
</p><p>
The <code>mini-coder</code> models deliver SOTA performance on <a href="https://www.swebench.com/"></a>SWE-Bench Verified Bash-only</a> at their size. Remarkably, <code>mini-coder-4b</code> matches the performance of the much larger gpt-oss-120b, while <code>mini-coder-4b</code> outperforms SWE-Agent-LM 7B. The two models also achieve much higher pass@k than their corresponding base models. This indicates that the <code>mini-coder</code> models are strong candidates for RL fine-tuning, since pass@k reflects the fraction of problems from which effective supervision can be derived.
</p><p>
Unlike existing agentic SWE models, the <code>mini-coder</code> models can be post-trained on a single 80GB GPU—or smaller. They integrate seamlessly with mini-swe-agent, a lightweight, scalable, and developer-friendly agentic framework well-suited for RL fine-tuning. And because they are dense rather than MoE models, they benefit from a more mature fine-tuning ecosystem. Additionally, researchers can incorportate our <a href="https://huggingface.co/datasets/ricdomolm/mini-coder-trajs-400k">dataset</a> of 400k training trajectories in their post-training recipes. All in all, we hope that the mini-coder models will accelerate progress in agentic SWE research.
</p><p>
<br>
<br>
<br>
<br>
<br>
<br>
</span>
<?xml version="1.0" encoding="UTF-8"?>
<html>
<head>
  <title>How to Fix the Chatbot Arena? Release All Data</title>
  <style type="text/css">
    body {
      text-align: center;
      max-width: 800px;
      margin: 0 auto;
      line-height: 1.6;
    }
    p {
      margin: 1em auto;
      max-width:80ch; /* Limits line length to approximately 80 characters */
      text-align: justify;
    }
  </style>
</head>
<body>
<span class="style1">
<h1>How to Fix the Chatbot Arena? Release All Data<br>
</h1>
<h2>Ricardo Dominguez-Olmedo</h2>
<h3>May 2, 2025<br>
</h3>
<br>
<div class="centered-text" style="max-width: 65ch; margin: 0 auto;">
<i>Releasing all Arena data would make LMArena a fairer leaderboard. It would also force us to reflect on whether we actually care who is at the top.</i>
</div>
<!-- &quot; -->
<p>
<br>
The Chatbot Arena leaderboard has recently become the center of controversy. <a href="https://arxiv.org/abs/2504.20879">Researchers have recently shown</a> that some model providers have been able to collect significantly more Arena data than others. They argue that this asymmetry in data access gives certain providers an unfair competitive advantage.
</p><p>
<a href="https://x.com/lmarena_ai/status/1917492084359192890">In response</a>, the Arena organizers argue that access to Arena data is a positive thing —it helps providers optimize for millions of people&apos;s preferences. They further claim that if one provider collects more data than another, that&apos;s not necessarily unfair. After all, Provider B could, in principle, have employed the same data collection strategies as Provider A.
</p><p>
At the heart of this debate lies a subtle but critical issue: training on the test task. Unlike training on the test set—which is clearly problematic—training on the test task is a legitimate way to improve performance. f the goal is to better align models with human preferences, why not train on as much preference data as possible? At the same time, if a handful of providers have access to vastly more Arena than everyone else, how can others hope to compete?
</p><p>
The researchers proposed to address such concerns by limiting how much Arena data any single provider can collect. But this doesn&apos;t necessarily level the playing field. Providers who already collected large amounts of data would retain their advantages. Those with more models on the Arena could still collect fresh data at higher rates. And well-resourced industry labs can obtain vast amounts of additional human preference data by other means. So, what can we do instead?
</p><p>
<b>LMArena should release all Arena data.</b> Doing so would ensure that every future submission has access to the same amount of Arena data — all of it. Given its scale, additional human preference data that well-resourced labs may be able to collect would have diminishing value. The broader research community would benefit, since large real-world preference datasets are scarce.
</p><p>
This approach aligns with the conclusions of our recent paper on &quot;<a href="https://arxiv.org/abs/2407.07890">training on the test task</a>&quot;. The best way to fight training on the test task is to embrace it. Unfairness in benchmarking arises only if some providers can train on the test task much more than others. By releasing all Arena data, we can hope to restore fair rankings by ensuring everyone can benefit as much as possible from the data.
</p><p>
Embracing training on the test task has an additional benefit. By putting more pressure on benchmarks, it forces us to reflect on how much we should care about them. This intuition often manifests itself through the specter of &quot;overfitting&quot;. But the Arena tests models on fresh data. As the organizers put it, &quot;If a model does well on LMArena, it means that our community likes it!&quot; And that&apos;s exactly what it means. Whether we choose to read too much into it is on us.
</p><p>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
</span>